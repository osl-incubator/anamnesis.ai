{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#anamnesisai","title":"anamnesis.ai","text":"<p>An AI-driven anamnesis collection system in the healthcare domain. The system will leverage the capabilities of FHIR (Fast Healthcare Interoperability Resources), the ChatGPT API, Flask (a micro web framework written in Python), and SQLite (a lightweight database) to facilitate an interactive, user-friendly platform for collecting patient medical history (anamnesis) through conversational AI.</p> <ul> <li>License: BSD 3 Clause</li> <li>Documentation: https://osl-incubator.github.io/anamnesisai</li> </ul>"},{"location":"changelog/","title":"Release Notes","text":""},{"location":"changelog/#020-2024-12-11","title":"0.2.0 (2024-12-11)","text":""},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Add support to more FHIR resources and improve the organization of the modules (#20) (69e4bfa)</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>In order to be able to contribute, it is important that you understand the project layout. This project uses the src layout, which means that the package code is located at <code>./src/anamnesisai</code>.</p> <p>For my information, check the official documentation: https://packaging.python.org/en/latest/discussions/src-layout-vs-flat-layout/</p> <p>In addition, you should know that to build our package we use Poetry, it's a Python package management tool that simplifies the process of building and publishing Python packages. It allows us to easily manage dependencies, virtual environments and package versions. Poetry also includes features such as dependency resolution, lock files and publishing to PyPI. Overall, Poetry streamlines the process of managing Python packages, making it easier for us to create and share our code with others.</p> <p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at /issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \u201cbug\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \u201cenhancement\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>anamnesis.ai could always use more documentation, whether as part of the official anamnesis.ai docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at /issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are   welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here\u2019s how to set up <code>anamnesis.ai</code> for local development.</p> <ol> <li> <p>Fork the <code>anamnesis.ai</code> repo on GitHub.</p> </li> <li> <p>Clone your fork locally::</p> <p>$ git clone git@github.com:your_name_here/anamnesis.ai.git</p> </li> <li> <p>Install your local copy into a conda environment. Assuming you have conda or     mamba installed, this is how you set up your fork for local development     (ensure you are already in the anamnesis.ai folder):     <pre><code>$ mamba env create --file conda/dev.yaml\n$ conda activate anamnesisai\n</code></pre></p> </li> <li>Install the dependencies in the new environment anamnesisai:     <pre><code>$ poetry install\n</code></pre></li> <li> <p>Create a branch for local development::</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you\u2019re done making changes, check that your changes pass flake8 and the     tests, including testing other Python versions with tox::</p> <pre><code>$ makim tests.linter\n$ makim tests.unit\n</code></pre> </li> <li> <p>Commit your changes and push your branch to GitHub::</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put your     new functionality into a function with a docstring, and add the feature to     the list in README.rst.</li> <li>The pull request should work for Python &gt;= <code>3.9</code>.</li> </ol>"},{"location":"contributing/#tips","title":"Tips","text":"<p>To run a subset of tests::</p> <pre><code>$ pytest tests.test_anamnesisai\n</code></pre>"},{"location":"contributing/#release","title":"Release","text":"<p>This project uses semantic-release in order to cut a new release based on the commit-message.</p>"},{"location":"contributing/#commit-message-format","title":"Commit message format","text":"<p>semantic-release uses the commit messages to determine the consumer impact of changes in the codebase. Following formalized conventions for commit messages, semantic-release automatically determines the next semantic version number, generates a changelog and publishes the release.</p> <p>By default, semantic-release uses Angular Commit Message Conventions. The commit message format can be changed with the <code>preset</code> or <code>config</code> options_ of the @semantic-release/commit-analyzer and @semantic-release/release-notes-generator plugins.</p> <p>Tools such as commitizen or commitlint can be used to help contributors and enforce valid commit messages.</p> <p>The table below shows which commit message gets you which release type when <code>semantic-release</code> runs (using the default configuration):</p> Commit message Release type <code>fix(pencil): stop graphite breaking when pressure is applied</code> Fix Release <code>feat(pencil): add 'graphiteWidth' option</code> Feature Release <code>perf(pencil): remove graphiteWidth option</code> Chore <code>feat(pencil)!: The graphiteWidth option has been removed</code> Breaking Release <p>source: https://github.com/semantic-release/semantic-release/blob/master/README.md#commit-message-format</p> <p>As this project uses the <code>squash and merge</code> strategy, ensure to apply the commit message format to the PR's title.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install anamnesis.ai, run this command in your terminal:</p> <pre><code>$ pip install anamnesisai\n</code></pre> <p>This is the preferred method to install anamnesis.ai, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>The sources for anamnesis.ai can be downloaded from the Github repo.</p> <p>You can either clone the public repository:</p> <pre><code>$ git clone\n</code></pre> <p>Or download the tarball:</p> <pre><code>$ curl -OJL /tarball/main\n</code></pre> <p>Once you have a copy of the source, you can install it with:</p> <pre><code>$ poetry install\n</code></pre>"},{"location":"notebook/rag-implementation-with-llama/","title":"Installations, imports, utils","text":"In\u00a0[1]: Copied! <pre>!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\nbitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12\n</pre> !pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\ bitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12 <pre>Collecting transformers==4.33.0\r\n</pre> <pre>  Downloading transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\r\n</pre> <pre>Collecting accelerate==0.22.0\r\n  Downloading accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\r\nCollecting einops==0.6.1\r\n</pre> <pre>  Downloading einops-0.6.1-py3-none-any.whl.metadata (12 kB)\r\n</pre> <pre>Collecting langchain==0.0.300\r\n</pre> <pre>  Downloading langchain-0.0.300-py3-none-any.whl.metadata (15 kB)\r\nCollecting xformers==0.0.21\r\n</pre> <pre>  Downloading xformers-0.0.21-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\r\nCollecting bitsandbytes==0.41.1\r\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\r\n</pre> <pre>Collecting sentence_transformers==2.2.2\r\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\r\n</pre> <pre>  Preparing metadata (setup.py) ... -</pre> <pre>\b \b\\</pre> <pre>\b \bdone\r\nCollecting chromadb==0.4.12\r\n</pre> <pre>  Downloading chromadb-0.4.12-py3-none-any.whl.metadata (7.0 kB)\r\nRequirement already satisfied: filelock in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from transformers==4.33.0) (3.16.1)\r\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.15.1 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from transformers==4.33.0) (0.26.5)\r\nRequirement already satisfied: numpy&gt;=1.17 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from transformers==4.33.0) (1.26.4)\r\nRequirement already satisfied: packaging&gt;=20.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from transformers==4.33.0) (24.2)\r\nRequirement already satisfied: pyyaml&gt;=5.1 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from transformers==4.33.0) (6.0.2)\r\nRequirement already satisfied: regex!=2019.12.17 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from transformers==4.33.0) (2024.11.6)\r\nRequirement already satisfied: requests in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from transformers==4.33.0) (2.32.3)\r\n</pre> <pre>Collecting tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1 (from transformers==4.33.0)\r\n  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\nRequirement already satisfied: safetensors&gt;=0.3.1 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from transformers==4.33.0) (0.4.5)\r\nRequirement already satisfied: tqdm&gt;=4.27 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from transformers==4.33.0) (4.67.1)\r\nRequirement already satisfied: psutil in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from accelerate==0.22.0) (6.1.0)\r\nRequirement already satisfied: torch&gt;=1.10.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from accelerate==0.22.0) (2.5.1+cpu)\r\nRequirement already satisfied: SQLAlchemy&lt;3,&gt;=1.4 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from langchain==0.0.300) (2.0.36)\r\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.3 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from langchain==0.0.300) (3.11.10)\r\n</pre> <pre>Collecting anyio&lt;4.0 (from langchain==0.0.300)\r\n  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\r\n</pre> <pre>Requirement already satisfied: dataclasses-json&lt;0.7,&gt;=0.5.7 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from langchain==0.0.300) (0.6.7)\r\nRequirement already satisfied: jsonpatch&lt;2.0,&gt;=1.33 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from langchain==0.0.300) (1.33)\r\n</pre> <pre>Collecting langsmith&lt;0.1.0,&gt;=0.0.38 (from langchain==0.0.300)\r\n  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\r\n</pre> <pre>Collecting numexpr&lt;3.0.0,&gt;=2.8.4 (from langchain==0.0.300)\r\n</pre> <pre>  Downloading numexpr-2.10.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\r\nRequirement already satisfied: pydantic&lt;3,&gt;=1 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from langchain==0.0.300) (2.10.3)\r\nCollecting tenacity&lt;9.0.0,&gt;=8.1.0 (from langchain==0.0.300)\r\n  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\r\n</pre> <pre>Collecting torch&gt;=1.10.0 (from accelerate==0.22.0)\r\n  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\r\nRequirement already satisfied: torchvision in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from sentence_transformers==2.2.2) (0.20.1+cpu)\r\nRequirement already satisfied: scikit-learn in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from sentence_transformers==2.2.2) (1.5.2)\r\nRequirement already satisfied: scipy in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from sentence_transformers==2.2.2) (1.13.1)\r\n</pre> <pre>Collecting nltk (from sentence_transformers==2.2.2)\r\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\r\nRequirement already satisfied: sentencepiece in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from sentence_transformers==2.2.2) (0.2.0)\r\n</pre> <pre>Collecting pydantic&lt;3,&gt;=1 (from langchain==0.0.300)\r\n  Downloading pydantic-1.10.19-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (152 kB)\r\n</pre> <pre>Collecting chroma-hnswlib==0.7.3 (from chromadb==0.4.12)\r\n  Downloading chroma_hnswlib-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\r\n</pre> <pre>Collecting fastapi&lt;0.100.0,&gt;=0.95.2 (from chromadb==0.4.12)\r\n  Downloading fastapi-0.99.1-py3-none-any.whl.metadata (23 kB)\r\n</pre> <pre>Collecting uvicorn&gt;=0.18.3 (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb==0.4.12)\r\n  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\r\n</pre> <pre>Collecting posthog&gt;=2.4.0 (from chromadb==0.4.12)\r\n  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\r\nRequirement already satisfied: typing-extensions&gt;=4.5.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from chromadb==0.4.12) (4.12.2)\r\n</pre> <pre>Collecting pulsar-client&gt;=3.1.0 (from chromadb==0.4.12)\r\n  Downloading pulsar_client-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\r\n</pre> <pre>Collecting onnxruntime&gt;=1.14.1 (from chromadb==0.4.12)\r\n  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\r\n</pre> <pre>Collecting pypika&gt;=0.48.9 (from chromadb==0.4.12)\r\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\r\n</pre> <pre>  Installing build dependencies ... -</pre> <pre>\b \b\\</pre> <pre>\b \b|</pre> <pre>\b \bdone\r\n</pre> <pre>  Getting requirements to build wheel ... -</pre> <pre>\b \bdone\r\n</pre> <pre>  Preparing metadata (pyproject.toml) ... -\b \bdone\r\n</pre> <pre>Collecting overrides&gt;=7.3.1 (from chromadb==0.4.12)\r\n  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\r\nRequirement already satisfied: importlib-resources in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from chromadb==0.4.12) (6.4.5)\r\n</pre> <pre>Collecting bcrypt&gt;=4.0.1 (from chromadb==0.4.12)\r\n  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\r\nRequirement already satisfied: typer&gt;=0.9.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from chromadb==0.4.12) (0.15.1)\r\nRequirement already satisfied: sympy in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from torch&gt;=1.10.0-&gt;accelerate==0.22.0) (1.13.1)\r\nRequirement already satisfied: networkx in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from torch&gt;=1.10.0-&gt;accelerate==0.22.0) (3.2.1)\r\nRequirement already satisfied: jinja2 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from torch&gt;=1.10.0-&gt;accelerate==0.22.0) (3.1.4)\r\n</pre> <pre>Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n</pre> <pre>Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n</pre> <pre>Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n</pre> <pre>Collecting nvidia-cufft-cu11==10.9.0.58 (from torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n</pre> <pre>Collecting nvidia-curand-cu11==10.2.10.91 (from torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\nCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n</pre> <pre>Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\nCollecting nvidia-nccl-cu11==2.14.3 (from torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n</pre> <pre>  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\r\nCollecting nvidia-nvtx-cu11==11.7.91 (from torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\r\n</pre> <pre>Collecting triton==2.0.0 (from torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\r\nRequirement already satisfied: setuptools in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch&gt;=1.10.0-&gt;accelerate==0.22.0) (75.6.0)\r\nRequirement already satisfied: wheel in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66-&gt;torch&gt;=1.10.0-&gt;accelerate==0.22.0) (0.45.1)\r\n</pre> <pre>Collecting cmake (from triton==2.0.0-&gt;torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n  Downloading cmake-3.31.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\r\nCollecting lit (from triton==2.0.0-&gt;torch&gt;=1.10.0-&gt;accelerate==0.22.0)\r\n</pre> <pre>  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\r\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.300) (2.4.4)\r\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.300) (1.3.1)\r\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.300) (24.2.0)\r\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.300) (1.5.0)\r\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.300) (6.1.0)\r\nRequirement already satisfied: propcache&gt;=0.2.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.300) (0.2.1)\r\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.300) (1.18.3)\r\nRequirement already satisfied: idna&gt;=2.8 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from anyio&lt;4.0-&gt;langchain==0.0.300) (3.10)\r\nRequirement already satisfied: sniffio&gt;=1.1 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from anyio&lt;4.0-&gt;langchain==0.0.300) (1.3.1)\r\nRequirement already satisfied: marshmallow&lt;4.0.0,&gt;=3.18.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from dataclasses-json&lt;0.7,&gt;=0.5.7-&gt;langchain==0.0.300) (3.23.1)\r\nRequirement already satisfied: typing-inspect&lt;1,&gt;=0.4.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from dataclasses-json&lt;0.7,&gt;=0.5.7-&gt;langchain==0.0.300) (0.9.0)\r\n</pre> <pre>Collecting starlette&lt;0.28.0,&gt;=0.27.0 (from fastapi&lt;0.100.0,&gt;=0.95.2-&gt;chromadb==0.4.12)\r\n</pre> <pre>  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\r\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from huggingface-hub&lt;1.0,&gt;=0.15.1-&gt;transformers==4.33.0) (2024.10.0)\r\nRequirement already satisfied: jsonpointer&gt;=1.9 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from jsonpatch&lt;2.0,&gt;=1.33-&gt;langchain==0.0.300) (3.0.0)\r\n</pre> <pre>Collecting coloredlogs (from onnxruntime&gt;=1.14.1-&gt;chromadb==0.4.12)\r\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\nCollecting flatbuffers (from onnxruntime&gt;=1.14.1-&gt;chromadb==0.4.12)\r\n</pre> <pre>  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\r\nRequirement already satisfied: protobuf in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from onnxruntime&gt;=1.14.1-&gt;chromadb==0.4.12) (5.29.1)\r\nRequirement already satisfied: six&gt;=1.5 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from posthog&gt;=2.4.0-&gt;chromadb==0.4.12) (1.17.0)\r\nCollecting monotonic&gt;=1.5 (from posthog&gt;=2.4.0-&gt;chromadb==0.4.12)\r\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\r\n</pre> <pre>Collecting backoff&gt;=1.10.0 (from posthog&gt;=2.4.0-&gt;chromadb==0.4.12)\r\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\r\nRequirement already satisfied: python-dateutil&gt;2.1 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from posthog&gt;=2.4.0-&gt;chromadb==0.4.12) (2.9.0.post0)\r\nRequirement already satisfied: certifi in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from pulsar-client&gt;=3.1.0-&gt;chromadb==0.4.12) (2024.8.30)\r\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from requests-&gt;transformers==4.33.0) (3.4.0)\r\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from requests-&gt;transformers==4.33.0) (2.2.3)\r\nRequirement already satisfied: greenlet!=0.4.17 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from SQLAlchemy&lt;3,&gt;=1.4-&gt;langchain==0.0.300) (3.1.1)\r\n</pre> <pre>Requirement already satisfied: click&gt;=8.0.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from typer&gt;=0.9.0-&gt;chromadb==0.4.12) (8.1.7)\r\nRequirement already satisfied: shellingham&gt;=1.3.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from typer&gt;=0.9.0-&gt;chromadb==0.4.12) (1.5.4)\r\nRequirement already satisfied: rich&gt;=10.11.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from typer&gt;=0.9.0-&gt;chromadb==0.4.12) (13.9.4)\r\nRequirement already satisfied: h11&gt;=0.8 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from uvicorn&gt;=0.18.3-&gt;uvicorn[standard]&gt;=0.18.3-&gt;chromadb==0.4.12) (0.14.0)\r\nCollecting httptools&gt;=0.6.3 (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb==0.4.12)\r\n</pre> <pre>  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\r\nRequirement already satisfied: python-dotenv&gt;=0.13 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb==0.4.12) (1.0.1)\r\n</pre> <pre>Collecting uvloop!=0.15.0,!=0.15.1,&gt;=0.14.0 (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb==0.4.12)\r\n  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\r\n</pre> <pre>Collecting watchfiles&gt;=0.13 (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb==0.4.12)\r\n  Downloading watchfiles-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\r\n</pre> <pre>Collecting websockets&gt;=10.4 (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb==0.4.12)\r\n  Downloading websockets-14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\nRequirement already satisfied: joblib in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from nltk-&gt;sentence_transformers==2.2.2) (1.4.2)\r\n</pre> <pre>Requirement already satisfied: threadpoolctl&gt;=3.1.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from scikit-learn-&gt;sentence_transformers==2.2.2) (3.5.0)\r\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\r\n</pre> <pre>Collecting torchvision (from sentence_transformers==2.2.2)\r\n  Downloading torchvision-0.20.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\r\n  Downloading torchvision-0.20.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\r\n</pre> <pre>  Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\r\n  Downloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\r\n  Downloading torchvision-0.18.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\r\n</pre> <pre>  Downloading torchvision-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\r\n  Downloading torchvision-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\r\nINFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\r\n  Downloading torchvision-0.17.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\r\n</pre> <pre>  Downloading torchvision-0.17.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\r\n  Downloading torchvision-0.16.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\r\n  Downloading torchvision-0.16.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\r\n</pre> <pre>  Downloading torchvision-0.16.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\r\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\r\n  Downloading torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (11 kB)\r\nRequirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from torchvision-&gt;sentence_transformers==2.2.2) (11.0.0)\r\n</pre> <pre>Requirement already satisfied: markdown-it-py&gt;=2.2.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from rich&gt;=10.11.0-&gt;typer&gt;=0.9.0-&gt;chromadb==0.4.12) (3.0.0)\r\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from rich&gt;=10.11.0-&gt;typer&gt;=0.9.0-&gt;chromadb==0.4.12) (2.18.0)\r\nRequirement already satisfied: mypy-extensions&gt;=0.3.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from typing-inspect&lt;1,&gt;=0.4.0-&gt;dataclasses-json&lt;0.7,&gt;=0.5.7-&gt;langchain==0.0.300) (1.0.0)\r\n</pre> <pre>Collecting humanfriendly&gt;=9.1 (from coloredlogs-&gt;onnxruntime&gt;=1.14.1-&gt;chromadb==0.4.12)\r\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\n</pre> <pre>Requirement already satisfied: MarkupSafe&gt;=2.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from jinja2-&gt;torch&gt;=1.10.0-&gt;accelerate==0.22.0) (3.0.2)\r\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from sympy-&gt;torch&gt;=1.10.0-&gt;accelerate==0.22.0) (1.3.0)\r\nRequirement already satisfied: mdurl~=0.1 in /home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich&gt;=10.11.0-&gt;typer&gt;=0.9.0-&gt;chromadb==0.4.12) (0.1.2)\r\n</pre> <pre>Downloading transformers-4.33.0-py3-none-any.whl (7.6 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/7.6 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.6/7.6 MB 120.6 MB/s eta 0:00:00\r\nDownloading accelerate-0.22.0-py3-none-any.whl (251 kB)\r\nDownloading einops-0.6.1-py3-none-any.whl (42 kB)\r\n</pre> <pre>Downloading langchain-0.0.300-py3-none-any.whl (1.7 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/1.7 MB ? eta -:--:--\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 101.2 MB/s eta 0:00:00\r\nDownloading xformers-0.0.21-cp311-cp311-manylinux2014_x86_64.whl (167.0 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/167.0 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 25.7/167.0 MB 127.8 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 43.0/167.0 MB 117.8 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 65.5/167.0 MB 108.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 87.0/167.0 MB 108.6 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 126.9/167.0 MB 126.1 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501 136.6/167.0 MB 113.0 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 165.7/167.0 MB 121.0 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 166.7/167.0 MB 115.7 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 167.0/167.0 MB 96.2 MB/s eta 0:00:00\r\nDownloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/92.6 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 36.4/92.6 MB 181.7 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 56.4/92.6 MB 140.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 68.2/92.6 MB 141.7 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501 77.6/92.6 MB 97.6 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 92.5/92.6 MB 102.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 92.6/92.6 MB 86.5 MB/s eta 0:00:00\r\nDownloading chromadb-0.4.12-py3-none-any.whl (426 kB)\r\n</pre> <pre>Downloading chroma_hnswlib-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/2.4 MB ? eta -:--:--\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.4/2.4 MB 115.4 MB/s eta 0:00:00\r\nDownloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/619.9 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 32.0/619.9 MB 159.6 MB/s eta 0:00:04</pre> <pre>\r   \u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 55.8/619.9 MB 139.0 MB/s eta 0:00:05</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 90.2/619.9 MB 149.8 MB/s eta 0:00:04</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 125.8/619.9 MB 156.6 MB/s eta 0:00:04</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 163.1/619.9 MB 162.1 MB/s eta 0:00:03</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 186.6/619.9 MB 154.6 MB/s eta 0:00:03</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 220.7/619.9 MB 156.7 MB/s eta 0:00:03</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 256.9/619.9 MB 159.8 MB/s eta 0:00:03</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 295.7/619.9 MB 163.6 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 335.8/619.9 MB 173.5 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 373.6/619.9 MB 175.5 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 412.1/619.9 MB 178.6 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 443.3/619.9 MB 171.6 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 483.1/619.9 MB 185.9 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501 520.6/619.9 MB 186.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501 554.7/619.9 MB 184.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501 586.7/619.9 MB 178.3 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 619.7/619.9 MB 176.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 619.7/619.9 MB 176.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 619.7/619.9 MB 176.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 619.7/619.9 MB 176.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 619.7/619.9 MB 176.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 619.7/619.9 MB 176.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 619.7/619.9 MB 176.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 619.7/619.9 MB 176.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 619.7/619.9 MB 176.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 619.7/619.9 MB 176.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 619.7/619.9 MB 176.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 619.7/619.9 MB 176.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 619.7/619.9 MB 176.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 619.9/619.9 MB 66.5 MB/s eta 0:00:00\r\nDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/317.1 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 44.0/317.1 MB 220.3 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 84.9/317.1 MB 212.5 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 127.1/317.1 MB 211.0 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 173.3/317.1 MB 215.6 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 216.0/317.1 MB 214.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501 257.7/317.1 MB 213.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501 298.1/317.1 MB 211.2 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 316.9/317.1 MB 210.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 316.9/317.1 MB 210.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 316.9/317.1 MB 210.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 316.9/317.1 MB 210.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 316.9/317.1 MB 210.5 MB/s eta 0:00:01\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 317.1/317.1 MB 120.1 MB/s eta 0:00:00\r\nDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/11.8 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11.8/11.8 MB 158.0 MB/s eta 0:00:00\r\nDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/21.0 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 21.0/21.0 MB 166.2 MB/s eta 0:00:00\r\nDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/849.3 kB ? eta -:--:--\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 849.3/849.3 kB 79.8 MB/s eta 0:00:00\r\n</pre> <pre>Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/557.1 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 34.6/557.1 MB 173.7 MB/s eta 0:00:04</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 73.9/557.1 MB 184.0 MB/s eta 0:00:03</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 110.1/557.1 MB 184.4 MB/s eta 0:00:03</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 145.2/557.1 MB 180.5 MB/s eta 0:00:03</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 182.7/557.1 MB 181.6 MB/s eta 0:00:03</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 212.9/557.1 MB 176.3 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 248.0/557.1 MB 175.9 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 287.8/557.1 MB 177.1 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 327.2/557.1 MB 179.5 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 328.2/557.1 MB 179.4 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 350.5/557.1 MB 149.9 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 390.9/557.1 MB 153.3 MB/s eta 0:00:02</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 435.2/557.1 MB 157.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501 476.1/557.1 MB 163.2 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501 512.0/557.1 MB 163.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501 542.1/557.1 MB 157.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 557.1/557.1 MB 156.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 557.1/557.1 MB 156.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 557.1/557.1 MB 156.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 557.1/557.1 MB 156.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 557.1/557.1 MB 156.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 557.1/557.1 MB 156.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 557.1/557.1 MB 156.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 557.1/557.1 MB 156.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 557.1/557.1 MB 156.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 557.1/557.1 MB 156.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 557.1/557.1 MB 156.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 557.1/557.1 MB 65.9 MB/s eta 0:00:00\r\nDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/168.4 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 37.7/168.4 MB 188.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 72.6/168.4 MB 180.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 116.4/168.4 MB 193.3 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501 158.3/168.4 MB 196.6 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 168.3/168.4 MB 198.9 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 168.4/168.4 MB 146.2 MB/s eta 0:00:00\r\nDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/54.6 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 32.2/54.6 MB 160.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 54.5/54.6 MB 136.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 54.6/54.6 MB 109.5 MB/s eta 0:00:00\r\nDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/102.6 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 38.8/102.6 MB 196.7 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 75.8/102.6 MB 188.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 102.5/102.6 MB 194.4 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 102.6/102.6 MB 143.5 MB/s eta 0:00:00\r\nDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/173.2 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 41.7/173.2 MB 207.9 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 89.1/173.2 MB 223.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 130.0/173.2 MB 215.6 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 173.0/173.2 MB 220.0 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 173.0/173.2 MB 220.0 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 173.2/173.2 MB 157.4 MB/s eta 0:00:00\r\nDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/177.1 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 46.7/177.1 MB 232.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 87.0/177.1 MB 216.9 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 125.8/177.1 MB 209.7 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501 164.1/177.1 MB 203.8 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 176.9/177.1 MB 203.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 176.9/177.1 MB 203.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 176.9/177.1 MB 203.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 176.9/177.1 MB 203.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 176.9/177.1 MB 203.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 176.9/177.1 MB 203.5 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 177.1/177.1 MB 79.9 MB/s eta 0:00:00\r\nDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\r\nDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/63.3 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 33.8/63.3 MB 171.7 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2578 63.2/63.3 MB 175.6 MB/s eta 0:00:01</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 63.3/63.3 MB 133.4 MB/s eta 0:00:00\r\nDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\r\nDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\r\n</pre> <pre>Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\r\nDownloading langsmith-0.0.92-py3-none-any.whl (56 kB)\r\nDownloading numexpr-2.10.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (398 kB)\r\nDownloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/13.3 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13.3/13.3 MB 170.6 MB/s eta 0:00:00\r\nDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\r\n</pre> <pre>Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\r\nDownloading pulsar_client-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/5.4 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 5.4/5.4 MB 124.6 MB/s eta 0:00:00\r\nDownloading pydantic-1.10.19-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/3.1 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.1/3.1 MB 138.5 MB/s eta 0:00:00\r\nDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\r\nDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/7.8 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.8/7.8 MB 174.1 MB/s eta 0:00:00\r\nDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\r\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/1.5 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.5/1.5 MB 109.8 MB/s eta 0:00:00\r\nDownloading torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl (6.0 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/6.0 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6.0/6.0 MB 129.8 MB/s eta 0:00:00\r\nDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\r\nDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\r\n</pre> <pre>Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\r\nDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\r\nDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/4.0 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 4.0/4.0 MB 130.2 MB/s eta 0:00:00\r\nDownloading watchfiles-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\r\nDownloading websockets-14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\r\n</pre> <pre>Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\nDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\r\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\nDownloading cmake-3.31.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.8 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/27.8 MB ? eta -:--:--</pre> <pre>\r   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 27.8/27.8 MB 153.7 MB/s eta 0:00:00\r\nDownloading lit-18.1.8-py3-none-any.whl (96 kB)\r\n</pre> <pre>Building wheels for collected packages: sentence_transformers, pypika\r\n</pre> <pre>  Building wheel for sentence_transformers (setup.py) ... -</pre> <pre>\b \b\\</pre> <pre>\b \bdone\r\n  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125922 sha256=a0665ab3a280270df57b47dd50ec0730af0dbd1dd5491bde78354abeefa36759\r\n  Stored in directory: /home/runner/.cache/pip/wheels/ff/27/bf/ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\r\n</pre> <pre>  Building wheel for pypika (pyproject.toml) ... -</pre> <pre>\b \bdone\r\n  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=334846e6677c8fd40aef9dfd0f3e508fe20df1b15e7f05d1c0b15aafac5d1480\r\n  Stored in directory: /home/runner/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\r\nSuccessfully built sentence_transformers pypika\r\n</pre> <pre>Installing collected packages: tokenizers, pypika, monotonic, lit, flatbuffers, bitsandbytes, websockets, uvloop, uvicorn, tenacity, pydantic, pulsar-client, overrides, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numexpr, nltk, humanfriendly, httptools, einops, cmake, chroma-hnswlib, bcrypt, backoff, anyio, watchfiles, starlette, posthog, nvidia-cusolver-cu11, nvidia-cudnn-cu11, langsmith, coloredlogs, transformers, onnxruntime, langchain, fastapi, chromadb, triton, torch, torchvision, xformers, sentence_transformers, accelerate\r\n  Attempting uninstall: tokenizers\r\n</pre> <pre>    Found existing installation: tokenizers 0.21.0\r\n    Uninstalling tokenizers-0.21.0:\r\n      Successfully uninstalled tokenizers-0.21.0\r\n</pre> <pre>  Attempting uninstall: tenacity\r\n    Found existing installation: tenacity 9.0.0\r\n    Uninstalling tenacity-9.0.0:\r\n      Successfully uninstalled tenacity-9.0.0\r\n  Attempting uninstall: pydantic\r\n    Found existing installation: pydantic 2.10.3\r\n    Uninstalling pydantic-2.10.3:\r\n      Successfully uninstalled pydantic-2.10.3\r\n</pre> <pre>  Attempting uninstall: anyio\r\n    Found existing installation: anyio 4.7.0\r\n    Uninstalling anyio-4.7.0:\r\n      Successfully uninstalled anyio-4.7.0\r\n</pre> <pre>  Attempting uninstall: langsmith\r\n    Found existing installation: langsmith 0.1.147\r\n</pre> <pre>    Uninstalling langsmith-0.1.147:\r\n      Successfully uninstalled langsmith-0.1.147\r\n</pre> <pre>  Attempting uninstall: transformers\r\n    Found existing installation: transformers 4.47.0\r\n</pre> <pre>    Uninstalling transformers-4.47.0:\r\n      Successfully uninstalled transformers-4.47.0\r\n</pre> <pre>  Attempting uninstall: langchain\r\n    Found existing installation: langchain 0.3.10\r\n</pre> <pre>    Uninstalling langchain-0.3.10:\r\n      Successfully uninstalled langchain-0.3.10\r\n</pre> <pre>  Attempting uninstall: torch\r\n    Found existing installation: torch 2.5.1+cpu\r\n</pre> <pre>    Uninstalling torch-2.5.1+cpu:\r\n      Successfully uninstalled torch-2.5.1+cpu\r\n</pre> <pre>  Attempting uninstall: torchvision\r\n    Found existing installation: torchvision 0.20.1+cpu\r\n    Uninstalling torchvision-0.20.1+cpu:\r\n      Successfully uninstalled torchvision-0.20.1+cpu\r\n</pre> <pre>  Attempting uninstall: sentence_transformers\r\n    Found existing installation: sentence-transformers 3.3.1\r\n    Uninstalling sentence-transformers-3.3.1:\r\n      Successfully uninstalled sentence-transformers-3.3.1\r\n</pre> <pre>ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nanamnesisai 0.1.0 requires torch&gt;=2.5.0, but you have torch 2.0.1 which is incompatible.\r\nanamnesisai 0.1.0 requires torchvision&gt;=0.20.0, but you have torchvision 0.15.2 which is incompatible.\r\nlangchain-community 0.3.10 requires langchain&lt;0.4.0,&gt;=0.3.10, but you have langchain 0.0.300 which is incompatible.\r\nlangchain-community 0.3.10 requires langsmith&lt;0.2.0,&gt;=0.1.125, but you have langsmith 0.0.92 which is incompatible.\r\ninstructor 1.7.0 requires pydantic&lt;3.0.0,&gt;=2.8.0, but you have pydantic 1.10.19 which is incompatible.\r\ninstructor 1.7.0 requires tenacity&lt;10.0.0,&gt;=9.0.0, but you have tenacity 8.5.0 which is incompatible.\r\nrago 0.9.0 requires langchain&gt;=0.3.7, but you have langchain 0.0.300 which is incompatible.\r\nrago 0.9.0 requires pydantic&gt;=2, but you have pydantic 1.10.19 which is incompatible.\r\nrago 0.9.0 requires sentence-transformers&gt;=3.2.0, but you have sentence-transformers 2.2.2 which is incompatible.\r\nlangchain-core 0.3.22 requires langsmith&lt;0.2.0,&gt;=0.1.125, but you have langsmith 0.0.92 which is incompatible.\r\nlangchain-core 0.3.22 requires pydantic&lt;3.0.0,&gt;=2.5.2; python_full_version &lt; \"3.12.4\", but you have pydantic 1.10.19 which is incompatible.\r\nfhir-core 0.1.3 requires pydantic&lt;3.0,&gt;=2.7.4, but you have pydantic 1.10.19 which is incompatible.\r\npydantic-settings 2.6.1 requires pydantic&gt;=2.7.0, but you have pydantic 1.10.19 which is incompatible.\r\nSuccessfully installed accelerate-0.22.0 anyio-3.7.1 backoff-2.2.1 bcrypt-4.2.1 bitsandbytes-0.41.1 chroma-hnswlib-0.7.3 chromadb-0.4.12 cmake-3.31.1 coloredlogs-15.0.1 einops-0.6.1 fastapi-0.99.1 flatbuffers-24.3.25 httptools-0.6.4 humanfriendly-10.0 langchain-0.0.300 langsmith-0.0.92 lit-18.1.8 monotonic-1.6 nltk-3.9.1 numexpr-2.10.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 onnxruntime-1.20.1 overrides-7.7.0 posthog-3.7.4 pulsar-client-3.5.0 pydantic-1.10.19 pypika-0.48.9 sentence_transformers-2.2.2 starlette-0.27.0 tenacity-8.5.0 tokenizers-0.13.3 torch-2.0.1 torchvision-0.15.2 transformers-4.33.0 triton-2.0.0 uvicorn-0.32.1 uvloop-0.21.0 watchfiles-1.0.3 websockets-14.1 xformers-0.0.21\r\n</pre> In\u00a0[2]: Copied! <pre>from torch import cuda, bfloat16\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer\nfrom time import time\n#import chromadb\n#from chromadb.config import Settings\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\n</pre> from torch import cuda, bfloat16 import torch import transformers from transformers import AutoTokenizer from time import time #import chromadb #from chromadb.config import Settings from langchain.llms import HuggingFacePipeline from langchain.document_loaders import TextLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.embeddings import HuggingFaceEmbeddings from langchain.chains import RetrievalQA from langchain.vectorstores import Chroma  <pre>/home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>Define the model, the device, and the <code>bitsandbytes</code> configuration.</p> In\u00a0[3]: Copied! <pre>model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)\n</pre> model_id = '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'  device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'  # set quantization configuration to load large model with less GPU memory # this requires the `bitsandbytes` library bnb_config = transformers.BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_quant_type='nf4',     bnb_4bit_use_double_quant=True,     bnb_4bit_compute_dtype=bfloat16 ) <p>Prepare the model and the tokenizer.</p> In\u00a0[4]: Copied! <pre>time_1 = time()\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntime_2 = time()\nprint(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")\n</pre> time_1 = time() model_config = transformers.AutoConfig.from_pretrained(     model_id, ) model = transformers.AutoModelForCausalLM.from_pretrained(     model_id,     trust_remote_code=True,     config=model_config,     quantization_config=bnb_config,     device_map='auto', ) tokenizer = AutoTokenizer.from_pretrained(model_id) time_2 = time() print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\") <pre>\n---------------------------------------------------------------------------\nHFValidationError                         Traceback (most recent call last)\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/transformers/configuration_utils.py in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs)\n    674                 # Load from local folder or from cache or download from model Hub and cache\n--&gt; 675                 resolved_config_file = cached_file(\n    676                     pretrained_model_name_or_path,\n\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/transformers/utils/hub.py in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\n    428         # Load from URL or cache if already cached\n--&gt; 429         resolved_file = hf_hub_download(\n    430             path_or_repo_id,\n\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py in _inner_fn(*args, **kwargs)\n    105             if arg_name in [\"repo_id\", \"from_id\", \"to_id\"]:\n--&gt; 106                 validate_repo_id(arg_value)\n    107 \n\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py in validate_repo_id(repo_id)\n    153     if repo_id.count(\"/\") &gt; 1:\n--&gt; 154         raise HFValidationError(\n    155             \"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\n\nHFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'. Use `repo_type` argument if needed.\n\nDuring handling of the above exception, another exception occurred:\n\nOSError                                   Traceback (most recent call last)\n/tmp/ipykernel_2690/1311373251.py in &lt;cell line: 0&gt;()\n      1 time_1 = time()\n----&gt; 2 model_config = transformers.AutoConfig.from_pretrained(\n      3     model_id,\n      4 )\n      5 model = transformers.AutoModelForCausalLM.from_pretrained(\n\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n   1021         revision = sanitize_code_revision(pretrained_model_name_or_path, revision, trust_remote_code)\n   1022 \n-&gt; 1023         config_dict, unused_kwargs = PretrainedConfig.get_config_dict(\n   1024             pretrained_model_name_or_path, revision=revision, **kwargs\n   1025         )\n\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)\n    618         original_kwargs = copy.deepcopy(kwargs)\n    619         # Get config dict associated with the base config file\n--&gt; 620         config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n    621         if \"_commit_hash\" in config_dict:\n    622             original_kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/transformers/configuration_utils.py in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs)\n    694             except Exception:\n    695                 # For any other exception, we throw a generic error.\n--&gt; 696                 raise EnvironmentError(\n    697                     f\"Can't load the configuration of '{pretrained_model_name_or_path}'. If you were trying to load it\"\n    698                     \" from 'https://huggingface.co/models', make sure you don't have a local directory with the same\"\n\nOSError: Can't load the configuration of '/kaggle/input/llama-2/pytorch/7b-chat-hf/1'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/kaggle/input/llama-2/pytorch/7b-chat-hf/1' is the correct path to a directory containing a config.json file</pre> <p>Define the query pipeline.</p> In\u00a0[5]: Copied! <pre>time_1 = time()\nquery_pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",)\ntime_2 = time()\nprint(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")\n</pre> time_1 = time() query_pipeline = transformers.pipeline(         \"text-generation\",         model=model,         tokenizer=tokenizer,         torch_dtype=torch.float16,         device_map=\"auto\",) time_2 = time() print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/tmp/ipykernel_2690/199530327.py in &lt;cell line: 0&gt;()\n      2 query_pipeline = transformers.pipeline(\n      3         \"text-generation\",\n----&gt; 4         model=model,\n      5         tokenizer=tokenizer,\n      6         torch_dtype=torch.float16,\n\nNameError: name 'model' is not defined</pre> <p>We define a function for testing the pipeline.</p> In\u00a0[6]: Copied! <pre>def test_model(tokenizer, pipeline, prompt_to_test):\n    \"\"\"\n    Perform a query\n    print the result\n    Args:\n        tokenizer: the tokenizer\n        pipeline: the pipeline\n        prompt_to_test: the prompt\n    Returns\n        None\n    \"\"\"\n    # adapted from https://huggingface.co/blog/llama2#using-transformers\n    time_1 = time()\n    sequences = pipeline(\n        prompt_to_test,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        max_length=200,)\n    time_2 = time()\n    print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n    for seq in sequences:\n        print(f\"Result: {seq['generated_text']}\")\n</pre> def test_model(tokenizer, pipeline, prompt_to_test):     \"\"\"     Perform a query     print the result     Args:         tokenizer: the tokenizer         pipeline: the pipeline         prompt_to_test: the prompt     Returns         None     \"\"\"     # adapted from https://huggingface.co/blog/llama2#using-transformers     time_1 = time()     sequences = pipeline(         prompt_to_test,         do_sample=True,         top_k=10,         num_return_sequences=1,         eos_token_id=tokenizer.eos_token_id,         max_length=200,)     time_2 = time()     print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")     for seq in sequences:         print(f\"Result: {seq['generated_text']}\") In\u00a0[7]: Copied! <pre>#test1\ntest_model(tokenizer,\n           query_pipeline,\n           \"Please explain what is the cause of diseases. Give just a definition. Keep it in 100 words.\")\n</pre> #test1 test_model(tokenizer,            query_pipeline,            \"Please explain what is the cause of diseases. Give just a definition. Keep it in 100 words.\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/tmp/ipykernel_2690/912279118.py in &lt;cell line: 0&gt;()\n      1 #test1\n----&gt; 2 test_model(tokenizer,\n      3            query_pipeline,\n      4            \"Please explain what is the cause of diseases. Give just a definition. Keep it in 100 words.\")\n\nNameError: name 'tokenizer' is not defined</pre> In\u00a0[8]: Copied! <pre>llm = HuggingFacePipeline(pipeline=query_pipeline)\n# test2\nllm(prompt=\"Please explain what is cancer? Give just a definition. Keep it in 100 words.\")\n</pre> llm = HuggingFacePipeline(pipeline=query_pipeline) # test2 llm(prompt=\"Please explain what is cancer? Give just a definition. Keep it in 100 words.\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/tmp/ipykernel_2690/696206675.py in &lt;cell line: 0&gt;()\n----&gt; 1 llm = HuggingFacePipeline(pipeline=query_pipeline)\n      2 # test2\n      3 llm(prompt=\"Please explain what is cancer? Give just a definition. Keep it in 100 words.\")\n\nNameError: name 'query_pipeline' is not defined</pre> In\u00a0[9]: Copied! <pre>loader = TextLoader(\"/kaggle/input/patient/Patient data.txt\",\n                    encoding=\"utf8\")\ndocuments = loader.load()\n</pre> loader = TextLoader(\"/kaggle/input/patient/Patient data.txt\",                     encoding=\"utf8\") documents = loader.load() <pre>\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/langchain/document_loaders/text.py in load(self)\n     39         try:\n---&gt; 40             with open(self.file_path, encoding=self.encoding) as f:\n     41                 text = f.read()\n\nFileNotFoundError: [Errno 2] No such file or directory: '/kaggle/input/patient/Patient data.txt'\n\nThe above exception was the direct cause of the following exception:\n\nRuntimeError                              Traceback (most recent call last)\n/tmp/ipykernel_2690/3108852290.py in &lt;cell line: 0&gt;()\n      1 loader = TextLoader(\"/kaggle/input/patient/Patient data.txt\",\n      2                     encoding=\"utf8\")\n----&gt; 3 documents = loader.load()\n\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/langchain/document_loaders/text.py in load(self)\n     54                 raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     55         except Exception as e:\n---&gt; 56             raise RuntimeError(f\"Error loading {self.file_path}\") from e\n     57 \n     58         metadata = {\"source\": self.file_path}\n\nRuntimeError: Error loading /kaggle/input/patient/Patient data.txt</pre> In\u00a0[10]: Copied! <pre>text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)\n</pre> text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20) all_splits = text_splitter.split_documents(documents) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/tmp/ipykernel_2690/3945965064.py in &lt;cell line: 0&gt;()\n      1 text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n----&gt; 2 all_splits = text_splitter.split_documents(documents)\n\nNameError: name 'documents' is not defined</pre> <p>Create the embeddings using Sentence Transformer and HuggingFace embeddings.</p> In\u00a0[11]: Copied! <pre>model_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n</pre> model_name = \"sentence-transformers/all-mpnet-base-v2\" model_kwargs = {\"device\": \"cuda\"}  embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs) <pre>\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/langchain/embeddings/huggingface.py in __init__(self, **kwargs)\n     57         try:\n---&gt; 58             import sentence_transformers\n     59 \n\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/sentence_transformers/__init__.py in &lt;module&gt;\n      2 __MODEL_HUB_ORGANIZATION__ = 'sentence-transformers'\n----&gt; 3 from .datasets import SentencesDataset, ParallelSentencesDataset\n      4 from .LoggingHandler import LoggingHandler\n\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/sentence_transformers/datasets/__init__.py in &lt;module&gt;\n      2 from .NoDuplicatesDataLoader import NoDuplicatesDataLoader\n----&gt; 3 from .ParallelSentencesDataset import ParallelSentencesDataset\n      4 from .SentencesDataset import SentencesDataset\n\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/sentence_transformers/datasets/ParallelSentencesDataset.py in &lt;module&gt;\n      3 import gzip\n----&gt; 4 from .. import SentenceTransformer\n      5 from ..readers import InputExample\n\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py in &lt;module&gt;\n     11 import transformers\n---&gt; 12 from huggingface_hub import HfApi, HfFolder, Repository, hf_hub_url, cached_download\n     13 import torch\n\nImportError: cannot import name 'cached_download' from 'huggingface_hub' (/home/runner/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/huggingface_hub/__init__.py)\n\nThe above exception was the direct cause of the following exception:\n\nImportError                               Traceback (most recent call last)\n/tmp/ipykernel_2690/2257289084.py in &lt;cell line: 0&gt;()\n      2 model_kwargs = {\"device\": \"cuda\"}\n      3 \n----&gt; 4 embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n~/miniconda3/envs/anamnesisai/lib/python3.11/site-packages/langchain/embeddings/huggingface.py in __init__(self, **kwargs)\n     59 \n     60         except ImportError as exc:\n---&gt; 61             raise ImportError(\n     62                 \"Could not import sentence_transformers python package. \"\n     63                 \"Please install it with `pip install sentence-transformers`.\"\n\nImportError: Could not import sentence_transformers python package. Please install it with `pip install sentence-transformers`.</pre> <p>Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally.</p> In\u00a0[12]: Copied! <pre>vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")\n</pre> vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/tmp/ipykernel_2690/756801264.py in &lt;cell line: 0&gt;()\n----&gt; 1 vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")\n\nNameError: name 'all_splits' is not defined</pre> In\u00a0[13]: Copied! <pre>retriever = vectordb.as_retriever()\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm, \n    chain_type=\"stuff\", \n    retriever=retriever, \n    verbose=True\n)\n</pre> retriever = vectordb.as_retriever()  qa = RetrievalQA.from_chain_type(     llm=llm,      chain_type=\"stuff\",      retriever=retriever,      verbose=True ) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/tmp/ipykernel_2690/736933915.py in &lt;cell line: 0&gt;()\n----&gt; 1 retriever = vectordb.as_retriever()\n      2 \n      3 qa = RetrievalQA.from_chain_type(\n      4     llm=llm,\n      5     chain_type=\"stuff\",\n\nNameError: name 'vectordb' is not defined</pre> In\u00a0[14]: Copied! <pre>def test_rag(qa, query):\n    print(f\"Query: {query}\\n\")\n    time_1 = time()\n    result = qa.run(query)\n    time_2 = time()\n    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n    print(\"\\nResult: \", result)\n</pre> def test_rag(qa, query):     print(f\"Query: {query}\\n\")     time_1 = time()     result = qa.run(query)     time_2 = time()     print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")     print(\"\\nResult: \", result) In\u00a0[15]: Copied! <pre>query = \"What were the main symptoms a patient with cough/cold was suffering from? Summarize. Keep it under 200 words.\"\ntest_rag(qa, query)\n</pre> query = \"What were the main symptoms a patient with cough/cold was suffering from? Summarize. Keep it under 200 words.\" test_rag(qa, query) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/tmp/ipykernel_2690/2636698936.py in &lt;cell line: 0&gt;()\n      1 query = \"What were the main symptoms a patient with cough/cold was suffering from? Summarize. Keep it under 200 words.\"\n----&gt; 2 test_rag(qa, query)\n\nNameError: name 'qa' is not defined</pre> In\u00a0[16]: Copied! <pre>query = \"What is the cause of headache? Summarize. Keep it under 200 words.\"\ntest_rag(qa, query)\n</pre> query = \"What is the cause of headache? Summarize. Keep it under 200 words.\" test_rag(qa, query) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/tmp/ipykernel_2690/2581198963.py in &lt;cell line: 0&gt;()\n      1 query = \"What is the cause of headache? Summarize. Keep it under 200 words.\"\n----&gt; 2 test_rag(qa, query)\n\nNameError: name 'qa' is not defined</pre> In\u00a0[17]: Copied! <pre>docs = vectordb.similarity_search(query)\nprint(f\"Query: {query}\")\nprint(f\"Retrieved documents: {len(docs)}\")\nfor doc in docs:\n    doc_details = doc.to_json()['kwargs']\n    print(\"Source: \", doc_details['metadata']['source'])\n    print(\"Text: \", doc_details['page_content'], \"\\n\")\n</pre> docs = vectordb.similarity_search(query) print(f\"Query: {query}\") print(f\"Retrieved documents: {len(docs)}\") for doc in docs:     doc_details = doc.to_json()['kwargs']     print(\"Source: \", doc_details['metadata']['source'])     print(\"Text: \", doc_details['page_content'], \"\\n\") <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/tmp/ipykernel_2690/641566907.py in &lt;cell line: 0&gt;()\n----&gt; 1 docs = vectordb.similarity_search(query)\n      2 print(f\"Query: {query}\")\n      3 print(f\"Retrieved documents: {len(docs)}\")\n      4 for doc in docs:\n      5     doc_details = doc.to_json()['kwargs']\n\nNameError: name 'vectordb' is not defined</pre>"},{"location":"notebook/rag-implementation-with-llama/#installations-imports-utils","title":"Installations, imports, utils\u00b6","text":""},{"location":"notebook/rag-implementation-with-llama/#initialize-model-tokenizer-query-pipeline","title":"Initialize model, tokenizer, query pipeline\u00b6","text":""},{"location":"notebook/rag-implementation-with-llama/#test-the-query-pipeline","title":"Test the query pipeline\u00b6","text":"<p>We test the pipeline with a query about the cause of diseases.</p>"},{"location":"notebook/rag-implementation-with-llama/#retrieval-augmented-generation","title":"Retrieval Augmented Generation\u00b6","text":""},{"location":"notebook/rag-implementation-with-llama/#check-the-model-with-a-huggingface-pipeline","title":"Check the model with a HuggingFace pipeline\u00b6","text":"<p>We check the model with a HF pipeline, using a query about cancer .</p>"},{"location":"notebook/rag-implementation-with-llama/#ingestion-of-data-using-text-loder","title":"Ingestion of data using Text loder\u00b6","text":""},{"location":"notebook/rag-implementation-with-llama/#split-data-in-chunks","title":"Split data in chunks\u00b6","text":"<p>We split data in chunks using a recursive character text splitter.</p>"},{"location":"notebook/rag-implementation-with-llama/#creating-embeddings-and-storing-in-vector-store","title":"Creating Embeddings and Storing in Vector Store\u00b6","text":""},{"location":"notebook/rag-implementation-with-llama/#initialize-chain","title":"Initialize chain\u00b6","text":""},{"location":"notebook/rag-implementation-with-llama/#test-the-retrieval-augmented-generation","title":"Test the Retrieval-Augmented Generation\u00b6","text":"<p>We define a test function, that will run the query and time it.</p>"},{"location":"notebook/rag-implementation-with-llama/#queries","title":"Queries.\u00b6","text":""},{"location":"notebook/rag-implementation-with-llama/#document-sources","title":"Document sources\u00b6","text":"<p>Let's check the documents sources, for the last query run.</p>"},{"location":"tutorials/llama/","title":"Llama","text":""},{"location":"tutorials/llama/#what-is-llama-2","title":"What is LLama-2?","text":"<p>LLama-2 represents a significant advancement in the development of large language models tailored for dialogue applications, offering state-of-the-art performance and encouraging responsible research practices within the community. It is a comprehensive collection of large language models (LLMs) developed and released by a team of researchers led by Hugo Touvron and Louis Martin. These models vary in scale, ranging from 7 billion to 70 billion parameters, and are specifically optimized for dialogue applications. Referred to as LLama 2-Chat, these fine-tuned LLMs have demonstrated superior performance compared to existing open-source chat models across various benchmarks.</p> <p>The research team conducted human evaluations focusing on the models' helpfulness and safety, indicating that LLama 2-Chat could serve as a viable alternative to closed-source models. They emphasize the importance of transparency and responsible development in the field of large language models, providing detailed descriptions of their fine-tuning process and safety enhancements to facilitate further research and community contributions.</p>"},{"location":"tutorials/llama/#steps-to-install-llama-2-models","title":"Steps to install Llama-2 models","text":"<p>To install Llama-2 models, follow these steps:</p> <ol> <li> <p>Visit the Llama    download form    on the official website and accept the License Agreement.</p> </li> <li> <p>Once your request is approved, you will receive a signed URL over email.    Check your email inbox (including spam/junk folders) for the email containing    the signed URL.</p> </li> <li> <p>Clone the Llama 2 repository from the provided link. You can typically do    this using a Git command like:</p> </li> </ol> <pre><code>git clone &lt;repository_url&gt;\n</code></pre> <p>Replace <code>&lt;repository_url&gt;</code> with the URL of the Llama 2 repository provided to    you.</p> <ol> <li> <p>Navigate to the directory where you cloned the Llama 2 repository.</p> </li> <li> <p>Run the <code>download.sh</code> script from the terminal, passing the signed URL    provided in the email when prompted to start the download. You can run the    script using the following command:</p> </li> </ol> <pre><code>bash download.sh\n</code></pre> <p>Follow the on-screen instructions and paste the signed URL when prompted.</p> <ol> <li> <p>Keep in mind that the signed URLs provided for downloading the models expire    after 24 hours and have a limited number of downloads. If you encounter    errors such as \"403: Forbidden\" during the download process, it likely means    that the URL has expired or reached its download limit.</p> </li> <li> <p>If you encounter such errors, you can always re-request a new signed URL by    going through the download form and accepting the License Agreement again.    Once you receive the new signed URL via email, repeat steps 3 to 5 to    download the models.</p> </li> </ol> <p>Following these steps should allow you to successfully install Llama-2 models for your blog.</p>"},{"location":"tutorials/llama/#starter-code","title":"Starter Code","text":"<pre><code>from transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nsequences = pipeline(\n    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n</code></pre>"}]}